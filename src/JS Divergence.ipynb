{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhargav/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "# from textstat.textstat import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    f1=\"../Proper Experiments/Parsed Datasets/\"+x+\".csv\"\n",
    "    f2=\"../Proper Experiments/Parsed Datasets/\"+y+\".csv\"\n",
    "    df = pd.read_csv(f1)\n",
    "    df.text=df.text.astype(str)\n",
    "    df_1 = pd.read_csv(f2)\n",
    "    df_1.text=df_1.text.astype(str)\n",
    "    corpus_1 = df['text'].values\n",
    "    corpus_2 = df_1['text'].values\n",
    "    text_1=\"\"\n",
    "    for i in corpus_1:\n",
    "        text_1=text_1+i\n",
    "    text_2=\"\"\n",
    "    for i in corpus_2:\n",
    "        text_2=text_2+i    \n",
    "    token = nltk.word_tokenize(text_1)\n",
    "    unigrams_1 = ngrams(token,1)\n",
    "\n",
    "    token = nltk.word_tokenize(text_2)\n",
    "    unigrams_2 = ngrams(token,1)\n",
    "\n",
    "    dict_1 = dict(Counter(unigrams_1))\n",
    "    dict_2 = dict(Counter(unigrams_2))\n",
    "\n",
    "    val=dict_1.values()\n",
    "    temp=sum(val)\n",
    "    for i in dict_1:\n",
    "        dict_1[i]=dict_1[i]/temp\n",
    "\n",
    "    val=dict_2.values()\n",
    "    temp=sum(val)\n",
    "    for i in dict_2:\n",
    "        dict_2[i]=dict_2[i]/temp\n",
    "\n",
    "    arr_1=[]\n",
    "    arr_2=[]\n",
    "    d1 = dict_1.copy()\n",
    "    d2 = dict_2.copy()\n",
    "    for i in dict_1:\n",
    "        if i in dict_2:\n",
    "            arr_1.append(d1[i])\n",
    "            arr_2.append(d2[i])\n",
    "            del d1[i]\n",
    "            del d2[i]\n",
    "\n",
    "    for i in d1:\n",
    "        arr_1.append(d1[i])\n",
    "        arr_2.append(0)\n",
    "\n",
    "    for i in d2:\n",
    "        arr_2.append(d2[i])\n",
    "        arr_1.append(0)\n",
    "\n",
    "    #print(len(arr_1)==len(arr_2))\n",
    "    \n",
    "    return (JSD(arr_1, arr_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2708084591488323\n"
     ]
    }
   ],
   "source": [
    "print(JSD(arr_1, arr_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "file_names = [\"Davidson\",\"Waseem_Hovy\", \"OffensEval_2019\", \"hatEval_2019\", \"StormfrontWS\", \"GAB\", \"Reddit\"]\n",
    "ans = []\n",
    "for i in file_names:\n",
    "    temp = []\n",
    "    for j in file_names:\n",
    "        temp.append(func(i, j))\n",
    "    ans.append(temp)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.292601498947455, 0.33216137946538016, 0.29260680130263295, 0.3598562841511459, 0.2928151050464475, 0.29206498798796654], [0.292601498947455, 0.0, 0.2834768806080229, 0.26407920790111505, 0.308104609777129, 0.23729516834112968, 0.21335683064375438], [0.33216137946538005, 0.2834768806080232, 0.0, 0.2708084591488324, 0.2937540220387137, 0.20304373392863598, 0.19542382409915726], [0.2926068013026331, 0.26407920790111516, 0.27080845914883245, 0.0, 0.3043615836162452, 0.21793842857281986, 0.22896732173684153], [0.3598562841511461, 0.30810460977712895, 0.2937540220387137, 0.3043615836162449, 0.0, 0.217633688807724, 0.2165194257196177], [0.29281510504644764, 0.2372951683411298, 0.20304373392863634, 0.2179384285728197, 0.217633688807724, 0.0, 0.11072327568462606], [0.2920649879879663, 0.21335683064375466, 0.19542382409915707, 0.22896732173684153, 0.21651942571961794, 0.11072327568462607, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=pd.DataFrame(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_csv(\"Divergence Values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
